

Information theory is an area of mathematics that has applications in biology, medicine, sociology, and psychology, medical science, etc.Using the probability and statistics we can measure the information flow. 
\section{Entropy}
\hspace{4em}
Now we want to introduce the concept of Shannon entropy or simply say that entropy, which is a measure of the uncertainty of a random variable. Let $X$ be a random variable 
\begin{definition}[Entropy]
 The entropy $H (X)$ of a discrete random variable $ X$ is
defined b
\end{definition} 


Higher entropy →Higher Uncertainty \\
Higher Entropy → More Amount of information \\
Measure information in Terms of Uncertainty \\
Uniform Distribution Entropy Will be maximum

\subsection{Write Here Your First Subsection Heading}
\hspace{4em}
Text in the 
\hspace{4em}
Text in the  
\hspace{4em}
Text in the  
